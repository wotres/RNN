{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.2985\n",
      "20 1.06822\n",
      "30 0.929451\n",
      "40 0.813408\n",
      "50 0.675963\n",
      "60 0.54799\n",
      "70 0.443279\n",
      "80 0.354706\n",
      "90 0.277612\n",
      "100 0.208841\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 털, 날개 데이터\n",
    "x_data = np.array(\n",
    "        [[0, 0],[1, 0],[1, 1],[0, 0],[0, 0],[0, 1]])\n",
    "\n",
    "# 기타, 포유류 , 조류 데이터\n",
    "y_data = np.array([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "# X 와 Y 에 실측값을 널어서 학습시킬 것이라 플레이스 홀더로 설정\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 가중치 W 는 [입력층(특징 수), 히든층(레이블 수]] 로 설정\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1., 1.))\n",
    "W2 = tf.Variable(tf.random_uniform([10,3], -1., 1.))\n",
    "\n",
    "# 편차 b 는 레이블 수인 3개의 요소를 가진 변수로 설정\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "L = tf.add(tf.matmul(X, W1), b1)\n",
    "# 활성화함수로 relu 함수 적용\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# softmax 함수는 다음처럼 배열 내의 결괏값들ㅇ르 전체 합이 1이 되도록 만들어줌\n",
    "# 즉, 전체가 1이니 각각은 해당 결과의 확률로 해석 가능\n",
    "# model = tf.nn.softmax(L)\n",
    "\n",
    "# 최종 모델\n",
    "model = tf.add(tf.matmul(L,W2),b2)\n",
    "\n",
    "# 손실 함수에 교차 엔트로피 함수 사용 -> 이는 예측값과 실제값 사이의 확률 분포 차이를 계산한 값\n",
    "# Y는 실측값, model 은 신경망을 통해 나온 예측값 , model 은 신경망을 통해 나온 예측값\n",
    "# 모델에 로그를 취한 뒤 곱한 값의 차원을 줄여 1차원으로 만듦\n",
    "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "# 텐서플로에서 제공한느 교차 엔트로피 함수 사용 v2를 붙이는게 최신 버젼\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits =model))\n",
    "\n",
    "# 경사하강법으로 최적화\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# 아담옵티마이저로 최적화 \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 텐서플로의 세션을 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 학습을 100번 진행\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict = {X: x_data, Y: y_data})\n",
    "    \n",
    "    # 학습 도중 10번에 한 번씩 손실값을 출력\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step +1, sess.run(cost, feed_dict ={X:x_data, Y:y_data}))\n",
    "        \n",
    "# 학습된 결과 확인\n",
    "# 요소 중 가장 큰 값의 인덱스를 찾아주는 argmax 함수를 사용하여 출력\n",
    "prediction = tf.argmax(model, axis =1)\n",
    "target = tf.argmax(Y, axis =1)\n",
    "print('예측값:', sess.run(prediction, feed_dict={X: x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "# 정확도 출력\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
