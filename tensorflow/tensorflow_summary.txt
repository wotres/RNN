tf.constant() : 상수를 변수에 저장
3 : 랭크가 0인 텐서, 세이프는 []
[1,2,3] : 랭크가 1인 텐서, 세이프는 [3]
[[1,2,3],[4,5,6]] : 랭크가 2인 텐서, 세이프는 [2,3]
[[[1,2,3]],[[7,8,9]]] : 랭크가 3인 텐서, 세이프는 [2,1,3]
랭크가 0이면 스칼라, 1이면 벡터, 2이면 행렬, 3 이상이면 n-Tensor 또는 n차원 텐서

텐서플로우는 그래프를 생성한 뒤 그래프를 실행시킴(sess.run(c))
tf.placeholder() 는 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수
선형회귀란 간단히 주어진 x와 y값을 가지고 서로 간의 관계를 파악하는것

tf.Variable(tf.random_uniform([1], -1.0, 1.0))
은 -1 ~ 1 사이의 균등ㅍ분포를 가진 무작위 값을 초기화 가능

손실함수(loss function)는 한쌍 (x, y)의 데이터에 대한 손실값을 계산하는 함수
손실값이란 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값 
이러한 손실값을 전체 데이터에 대해 구한 경우가 비용 -> 최소화 해야함
최적화 함수란 가중치와 편향 값을 변경해가면서 손실값을 최소화 하는 가장 최적화된 가중치와 편향 값을 찾아주는 함수-> 무작위로 하면 시간 너무 오래 걸ㄴ리니까 
경사하강법과 같은 알고리즘으로 빠르게 찾음
경사 하강법은 함수의 기울기를 구하고 기울기가 낮은 쪽으로 계속 이동시키면서 최적의 값을 찾아나가는 방법
최적화 함수의 매개 변수인 학습률(learning_rate)는 학습을 얼마나 급하게 할것인가를 설정이처럼 학습을 진행하는 과정에 영향을 주는 변수를 하이퍼 파라미터라고 함

Numpy는 수치해석용 파이썬 라이브러리 : 행렬조작과 연산에 유용
원핫 인코딩은 데이터가 가질 수 있는 값들을 일렬로 나열한 배열로 만들고 
그중 표현하려는 값을 뜻하는 인덱스만 원소로 1로 표기하고 나머지는 0으로 채우는 표기법

reduce_xxx  함수들은 텐서의 차원을 줄여줌
xxx 부분이 구체적인 차원 축소 방법을 뜻하고, axis 매개변수로 축소할 차원을 정함

np.loadtxt(unpack =True) 옵션은 과 np,tranpose 옵션은 행과 열을 바꿈
[[1,2,3]
 [1,2,3]] 
을 
[[1,1]
[2,2]
[3,3]]
으로 바꿔줌

과적합 문제를 해결하기 위해서가장 효과가 좋은 방법중 하나가 드롭아웃
드롭아웃은 학습시 전체 신경망 중 일부만을 사용하도록 하는것
즉, 학습 단계마다 일부 뉴런을 제거 함으로써, 일부 특징이 특정 뉴런들에 고정되는것을 막아 
가중치의 균형을 잡도록하여 과적합을 방지
하지만 충분히 학습되기까지 시간이 조금 더 걸림
-> 최근에는 배치 정규화라는 기법이 많이 이용 -> 과적합을 막아줄 뿐만 아니라 학습 속도도 향상시켜 주는 장점이 있음
tf.nn.batch_normalization과 tf.layers.batch_normalization함수로 쉽게 적용 가능
ex) tf.layers.batch_normalization(L1, training=is_training)

CNN 모델은 기본적으로 
컨볼루션 계층(합성곱 계층)과 풀링 계층으로 구성
지정한 크기를 윈도우 , 이 윈도우(3*3)의 값을 한칸씩움직이면서 은닉층을 완성
이때 몇칸씩 움직일지 정하는 값을 스트라이드라고 함
컨볼루션 계층에는 윈도우 크기만큼의 가중치와 1개의 편향을 적용, 이를 커널 또는 필터라고 함

오토인코더는 비지도 학습의 일종으로 입력값으로 부터 데이터의 특징을 찾아내는 학습방법
오토인코더는 들어온 데이터를 인코더를 통해 은닉층으로 내보내고 은닉층의 데이터를 디코더를 통해 출력층으로 내보낸 뒤, 
만들어진 출력값을 입력값과 비슷해지도록 만드는 가중치를 찾아내는 것

GAN(Generative Adversarial NetWork)
서로 대립하는 두 신경망을 경쟁시켜가며 결과물 생성 방법을 학습하는것
ex) 위조 지폐범은 경찰을 최대한 속이려고 노력하고, 경찰을 위조한 지폐를 최대한 감별하려고 노력함
이 과정에서 서로의 능력이 발전하고 위조지폐범은 진짜와 거의 구분할 수 없을 정도로 진자 같은 위조지폐를 만들 수 있음
실제 구조 및 순서
1. 실제 이미지를 주고 구분자(Discriminator)에게 이 이미지가 진짜임을 판단하게 함
2. 생성자(Generator)를 통해 노이즈로부터 임의의 이미지를 만들고 이것을 다시 같은 구분자를 통해 진짜 이미지인지를 판단

RNN(Recurrent Neural Network)
자연어 처리나 음성 인식처럼 순서가 있는 데이터를 처리하는데 강점을 가진 신경망
그림 가운데에 있는 한덩어리의 신경망을 셀(Cell)이라고 함
이 셀을 여러개 중첩하여 심층 신경망을 만듬

Seq2Seq 
순차적인 정보를 입력받는 신경망과 출력하는 신경망을 조합한 모델

아래 에러 발생시에
ValueError: Variable encode/rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

tf.reset_default_graph()
를 앞에 넣어주면됨

Inception 모델
구글에서 제공하는 높은 성능의 이미지 처리 신경망
꽃사진 
http://download.tensorflow.org/example_images/flower_photos.tgz
학습스크립트
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/image_retraining

학습시키기
python retrain.py \
    --bottleneck_dir=./bottlenecks \
    --model_dir=./inception \
    --output_graph=./flowers_graph.pb \
    --output_labels=./flowers_labels.txt \
    --image_dir ./flower_photos \
    --how_many_training_steps 1000

옵션 설명
--bottleneck_dir : 학습할 사진을 인셉션 용으로 변환해서 저장할 폴더
--model_dir : inception 모델을 다운로드 할 경로
--image_dir : 원본 이미지 경로
--output_graph : 추론에 사용할 학습된 파일(.pb) 경로
--output_labels : 추론에 사용할 레이블 파일 경로
--how_many_training_steps : 얼만큼 반복 학습시킬 것인지

예측 실행
python predict.py ./flower_photos/roses/3065719996_c16ecd5551.jpg

강화학습(DQN)
어떤 환경에서 인공지능 에이전트가 현재 상태를 판단하여 가장 이로운 행동을 하게 만드는 학습 방법
-> 학습의 불안정함을 막기위해 
1. 과거의 상태를 기억한 뒤 그중에서 임의이ㅡ 상태를 뽑아 학습시키는 방법을 사용 -> 치우치지 않도록 학습
2. 손실값 계산을 위해 학습을 진행하면서 최적의 행동을 얻어내는 기본신경망과 얻어낸 값이 좋은 선택인지 비교하는
목표신경망을 분리, 그리고 목표신경망은 계속 갱신하는 것이 아니라 기본 신경망의 학습된 결과값을 일정 주기마다 목표 신경망에 갱신해줌
