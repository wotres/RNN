{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-1d2d164a8de8>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/actmember/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/actmember/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/actmember/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/actmember/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/actmember/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0000 D_loss: 0.003359 G_loss: 7.903\n",
      "Epoch: 0001 D_loss: 0.01768 G_loss: 7.374\n",
      "Epoch: 0002 D_loss: 0.01934 G_loss: 7.3\n",
      "Epoch: 0003 D_loss: 0.01058 G_loss: 7.417\n",
      "Epoch: 0004 D_loss: 0.007963 G_loss: 8.626\n",
      "Epoch: 0005 D_loss: 0.03832 G_loss: 7.559\n",
      "Epoch: 0006 D_loss: 0.03309 G_loss: 7.197\n",
      "Epoch: 0007 D_loss: 0.0788 G_loss: 7.754\n",
      "Epoch: 0008 D_loss: 0.06115 G_loss: 8.034\n",
      "Epoch: 0009 D_loss: 0.1243 G_loss: 4.849\n",
      "Epoch: 0010 D_loss: 0.1975 G_loss: 5.353\n",
      "Epoch: 0011 D_loss: 0.3542 G_loss: 6.262\n",
      "Epoch: 0012 D_loss: 0.2964 G_loss: 4.542\n",
      "Epoch: 0013 D_loss: 0.1973 G_loss: 4.146\n",
      "Epoch: 0014 D_loss: 0.6161 G_loss: 4.125\n",
      "Epoch: 0015 D_loss: 0.5902 G_loss: 3.685\n",
      "Epoch: 0016 D_loss: 0.3492 G_loss: 3.663\n",
      "Epoch: 0017 D_loss: 0.5805 G_loss: 3.971\n",
      "Epoch: 0018 D_loss: 0.5669 G_loss: 2.845\n",
      "Epoch: 0019 D_loss: 0.9228 G_loss: 3.851\n",
      "Epoch: 0020 D_loss: 0.3895 G_loss: 3.314\n",
      "Epoch: 0021 D_loss: 0.6243 G_loss: 3.302\n",
      "Epoch: 0022 D_loss: 0.8353 G_loss: 2.987\n",
      "Epoch: 0023 D_loss: 0.6322 G_loss: 2.844\n",
      "Epoch: 0024 D_loss: 0.713 G_loss: 2.716\n",
      "Epoch: 0025 D_loss: 0.6796 G_loss: 2.755\n",
      "Epoch: 0026 D_loss: 0.8398 G_loss: 2.3\n",
      "Epoch: 0027 D_loss: 0.6275 G_loss: 2.568\n",
      "Epoch: 0028 D_loss: 0.8252 G_loss: 2.195\n",
      "Epoch: 0029 D_loss: 0.7432 G_loss: 2.383\n",
      "Epoch: 0030 D_loss: 0.7228 G_loss: 2.59\n",
      "Epoch: 0031 D_loss: 0.5968 G_loss: 2.81\n",
      "Epoch: 0032 D_loss: 0.621 G_loss: 2.764\n",
      "Epoch: 0033 D_loss: 0.6718 G_loss: 2.064\n",
      "Epoch: 0034 D_loss: 0.7098 G_loss: 2.313\n",
      "Epoch: 0035 D_loss: 0.8439 G_loss: 1.886\n",
      "Epoch: 0036 D_loss: 0.6972 G_loss: 2.003\n",
      "Epoch: 0037 D_loss: 0.6662 G_loss: 2.646\n",
      "Epoch: 0038 D_loss: 0.5815 G_loss: 2.409\n",
      "Epoch: 0039 D_loss: 0.6608 G_loss: 2.537\n",
      "Epoch: 0040 D_loss: 0.57 G_loss: 2.464\n",
      "Epoch: 0041 D_loss: 0.7632 G_loss: 2.421\n",
      "Epoch: 0042 D_loss: 0.8215 G_loss: 1.999\n",
      "Epoch: 0043 D_loss: 0.6239 G_loss: 2.41\n",
      "Epoch: 0044 D_loss: 0.6839 G_loss: 2.514\n",
      "Epoch: 0045 D_loss: 0.7269 G_loss: 2.033\n",
      "Epoch: 0046 D_loss: 0.7471 G_loss: 2.285\n",
      "Epoch: 0047 D_loss: 0.7232 G_loss: 2.355\n",
      "Epoch: 0048 D_loss: 0.6841 G_loss: 2.049\n",
      "Epoch: 0049 D_loss: 0.5729 G_loss: 2.237\n",
      "Epoch: 0050 D_loss: 0.6876 G_loss: 2.149\n",
      "Epoch: 0051 D_loss: 0.6031 G_loss: 2.077\n",
      "Epoch: 0052 D_loss: 0.6088 G_loss: 2.372\n",
      "Epoch: 0053 D_loss: 0.7241 G_loss: 2.188\n",
      "Epoch: 0054 D_loss: 0.808 G_loss: 2.139\n",
      "Epoch: 0055 D_loss: 0.851 G_loss: 2.32\n",
      "Epoch: 0056 D_loss: 0.6242 G_loss: 2.176\n",
      "Epoch: 0057 D_loss: 0.6308 G_loss: 2.394\n",
      "Epoch: 0058 D_loss: 0.6042 G_loss: 2.472\n",
      "Epoch: 0059 D_loss: 0.7031 G_loss: 2.331\n",
      "Epoch: 0060 D_loss: 0.6918 G_loss: 2.218\n",
      "Epoch: 0061 D_loss: 0.6857 G_loss: 2.278\n",
      "Epoch: 0062 D_loss: 0.7583 G_loss: 2.193\n",
      "Epoch: 0063 D_loss: 0.71 G_loss: 2.227\n",
      "Epoch: 0064 D_loss: 0.6294 G_loss: 2.605\n",
      "Epoch: 0065 D_loss: 0.7099 G_loss: 2.275\n",
      "Epoch: 0066 D_loss: 0.8107 G_loss: 2.025\n",
      "Epoch: 0067 D_loss: 0.5866 G_loss: 2.113\n",
      "Epoch: 0068 D_loss: 0.6414 G_loss: 2.085\n",
      "Epoch: 0069 D_loss: 0.7001 G_loss: 1.893\n",
      "Epoch: 0070 D_loss: 0.6369 G_loss: 2.089\n",
      "Epoch: 0071 D_loss: 0.8294 G_loss: 2.264\n",
      "Epoch: 0072 D_loss: 0.6448 G_loss: 2.156\n",
      "Epoch: 0073 D_loss: 0.6157 G_loss: 2.415\n",
      "Epoch: 0074 D_loss: 0.5807 G_loss: 2.178\n",
      "Epoch: 0075 D_loss: 0.8418 G_loss: 2.102\n",
      "Epoch: 0076 D_loss: 0.6642 G_loss: 2.398\n",
      "Epoch: 0077 D_loss: 0.6647 G_loss: 2.396\n",
      "Epoch: 0078 D_loss: 0.6413 G_loss: 2.686\n",
      "Epoch: 0079 D_loss: 0.7966 G_loss: 2.19\n",
      "Epoch: 0080 D_loss: 0.6218 G_loss: 2.162\n",
      "Epoch: 0081 D_loss: 0.7048 G_loss: 2.336\n",
      "Epoch: 0082 D_loss: 0.6559 G_loss: 2.057\n",
      "Epoch: 0083 D_loss: 0.5889 G_loss: 2.418\n",
      "Epoch: 0084 D_loss: 0.8983 G_loss: 2.16\n",
      "Epoch: 0085 D_loss: 0.7939 G_loss: 2.119\n",
      "Epoch: 0086 D_loss: 0.6031 G_loss: 2.116\n",
      "Epoch: 0087 D_loss: 0.6117 G_loss: 2.456\n",
      "Epoch: 0088 D_loss: 0.7434 G_loss: 2.351\n",
      "Epoch: 0089 D_loss: 0.7106 G_loss: 2.132\n",
      "Epoch: 0090 D_loss: 0.7606 G_loss: 2.045\n",
      "Epoch: 0091 D_loss: 0.6553 G_loss: 2.188\n",
      "Epoch: 0092 D_loss: 0.8159 G_loss: 2.191\n",
      "Epoch: 0093 D_loss: 0.731 G_loss: 2.029\n",
      "Epoch: 0094 D_loss: 0.707 G_loss: 2.241\n",
      "Epoch: 0095 D_loss: 0.7316 G_loss: 2.187\n",
      "Epoch: 0096 D_loss: 0.6724 G_loss: 2.206\n",
      "Epoch: 0097 D_loss: 0.8589 G_loss: 1.763\n",
      "Epoch: 0098 D_loss: 0.6465 G_loss: 2.039\n",
      "Epoch: 0099 D_loss: 0.5633 G_loss: 2.309\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "# 원하는 숫자 생성\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "# 하이퍼 파라미터들을 설정\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "# 신경망 모델 구성\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "# 기존에는 각 신경망의 변수들을 따로따로 학습해야했지만 \n",
    "# tf.layers를 사용하면 변수를 선언하지않고 tf.variable_scope를 이용해 스코프를 지정해줄 수 있음\n",
    "# tf.concat 함수를 이용해 noise 값에 labels 정보를 간단하게 추가\n",
    "# tf.layers.dense 함수를 이용해 은닉층을 만들고, 마찬가지로 진짜 이미지와 같은 크기의 값을 만드는 출력층(output)도 구성\n",
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        # noise 값에 labels 정보를 추가합니다.\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "\n",
    "        # TensorFlow 에서 제공하는 유틸리티 함수를 이용해 신경망을 매우 간단하게 구성할 수 있습니다.\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                 activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input,\n",
    "                                 activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        # 노이즈에서 생성한 이미지와 실제 이미지를 판별하는 모델의 변수를 동일하게 하기 위해,\n",
    "        # 이전에 사용되었던 변수를 재사용하도록 합니다.\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)        \n",
    "        # 손실값 계산에 sigmoid_cross_entropy_with_logits 함수를 사용하기 위해 None을 사용\n",
    "        output = tf.layers.dense(hidden, 1, activation=None)\n",
    "    return output\n",
    "\n",
    "# 노이즈를 균등분포로 생성하도록 작성\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])\n",
    "\n",
    "# 생성자를 구성한 뒤 진짜 이미지 데이터와 생성자가 만든 이미지 데이터를 이용하는 구분자를 하나씩 만들어줌\n",
    "# 생성자에는 레이블 정보를 추가하여 추후 레이블 정보에 해당하는 이미지를 생성할 수 있도록 유도\n",
    "# 가짜 이미지 구분자를 만들 떄는 진짜 이미지 구분자에서 사용한 변수들을 재사용하도록 reuse 옵션을 True로 설정\n",
    "G =generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n",
    "# loss_D_real은 D_real의 결괏값과 D_real의 크기만큼 1로 채운 값들을 비교(ones_like 함수)\n",
    "loss_D_real = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits = D_real, labels=tf.ones_like(D_real)))\n",
    "\n",
    "# loss_D_gene은 D_gene의 결괏값과 D_gene의 크기만큼 0으로 채운 값들을 비교하도록 함(zeros_like 함수)\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "\n",
    "# loss_D 값을 최소화 시키면 구분자(경찰)를 학습시킬 수 있음\n",
    "# D_real은 1에 가까워야하고 (실제 이미지는 진짜라고 판별), D_gene는 0에 가까워야 함(생성한 이미지는 가짜라고 판별)\n",
    "loss_D = loss_D_real + loss_D_gene\n",
    "\n",
    "# loss_G 를 구함 -> 생성자(위조지폐범)를 학습시키기 위한 손실값 D_gene를 1에 가깝게 만드는 값을 손실값을 취하도록 함\n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "# tf.get_collection 함수를 이용해 discriminator와 generator 스코프에서 사용된 변수들을 가져온 뒤, \n",
    "# 이변수들을 최적화에 사용할 각각으 ㅣ손실 함수와 함께 최적화 함수에 넣어 학습 모델 구성을 마무리\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list=vars_G)\n",
    "\n",
    "# 학습 시작\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Y: batch_ys, Z: noise})\n",
    "    \n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D_loss: {:.4}'.format(loss_val_D),\n",
    "          'G_loss: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    # 학습 결과를 확인\n",
    "    # 노이즈를 만들고 이것을 생성자 G에 넣어 결괏값을 만든 뒤 \n",
    "    # 이결과값들을 28*28 크기의 가짜 이미지로 만들어 samples 폴더에 저장하도록함\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict = {Y: mnist.test.labels[:sample_size], Z: noise})\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            \n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "        \n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)),\n",
    "                   bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('최적화 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
