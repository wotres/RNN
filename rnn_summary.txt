값 조정 하는 방법
학습률 : 어느정도로 오차를 반영할것인가 
오차가 발생하고 여기에 input 값을 나눈뒤 이 값을 더해줌
y = (A+dA)x
dA = L(E/x)
E = 정답 - 예측값

활성화 함수
시그모이드 함수 (로지스틱 함수)
y = 1/(1+e^-x)
입력값을 x에 입력

계층(layer) 속 존재하는 뉴런을 노드(node) 라고 함
각 노드들은 직전 직후 모든 뉴런들과 연결되어 있음
가중치로 영향력 조절

행렬을 이용한 곱
다음 노드 = 가중치 행렬 * 현재 노드행렬
X = W * I

역전파는 오차를 가중치(W) 비율에 따라 부여
이를 행렬 곱으로 나타낼때 분모는 일종의 정규화 인자로 판단하여 무시해도 일정 비율만 잃음
실제로 이런 오차는 다음 학습 단계에서 스스로 바로 잡아감
따라서 전치(transpose) 한 행렬을 에러에 곱해준다
before error = W^T * after error

경사하강법은 함수의 최저점을 구하기위한 좋은 접근 방법
오차함수로는 (목표값 - 실제값)^2 인 제곱 오차를 통해
최저점에 접근함에 따라 경사가 점점 작아짐 -> 목표물을 오버슈팅할 가능성이 작아짐

역전파를 통한 조정 방식
오차 함수 = n개의 노드에 대해 목표 값과 실제 값의 차를 구해 이를 제곱한 다음 모두를 더한 합
Tn = 정답 / On = 예측값
dE/dWjk = d/dWjk SIGMA(Tn-On)^2
여기서 아웃풋 연결된거만 영향을 각각 미친다. 따라서 n(전체) -> k(해당노드) 로 변경해도 무방
dE/dWjk = d/dWjk SIGMA(Tk-Ok)^2
연쇄법칙 이용
dE/dWjk = dE/dOk * dOk/dWjk
dE/dOk = -2(Tk-Ok)
Ok = SIGMOID(SIGMAj(Wjk*Oj))
dE/dWjk = -(Tk-Ok)*SIGMOID(SIGMAj(Wjk*Oj))(1-SIGMOID(SIGMAj(Wjk*Oj))*Oj

이수식을 거친 후 학습률을 설정해준 뒤 가중치 조절 
new Wjk = old Wjk - A*dE/dWjk

초기 가중치는 루뜨 시킨 노드의 개수의 역수
가중치를 상수로 같은 값으로 설정하거나 0으로 설정하면 안됨!
0.01~0.99가 적정
너무 크면 활성화 함수의 기울기가 매우 얕은 곳에 존재하여 가중치 업데이트 하는 학습이 저하됨

jupyter notebook
-> 해당 칸 : 셀
a 누르면 셀 생성

numpy : 배열 계산 모듈
numpy.zeros([2,2])

matplotlib.pyplot : 시각화 라이브러리
%matplotlib inline
matplotlib.pyplot.imshow(a, interpolation="nearest")
# interpolation 없음녀 부드럽게 처리

클래스는 정의 이며 객체는 그정의를 현실에서 구현한 인스턴스

신경망 기본 구성
초기화: 입력,은닉, 출력 노드 수 설정
학습: 학습데이터들을 통해 학습하고 가중치 업데이트
질의: 입력 받아 연산 후 출력 노드에서 답 전달


